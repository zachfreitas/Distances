{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMRhPrsAKEjZ6I5OuwXV1ff",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zachfreitas/Distances/blob/main/Distances.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distance Calculations"
      ],
      "metadata": {
        "id": "_Clk5V3WqZwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# libraries\n"
      ],
      "metadata": {
        "id": "BW-MROlqwWNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Euclidean Distance (L2 Norm)\n",
        "\n",
        "We start with the most common distance measure, namely Euclidean distance. It is a distance measure that best can be explained as the length of a segment connecting two points.\n",
        "\n",
        "The formula is rather straightforward as the distance is calculated from the cartesian coordinates of the points using the Pythagorean theorem.\n",
        "\n",
        "$$D(x,y) = \\sqrt{ \\sum_{i=1}^{n}(x_i-y_i)^2}$$\n",
        "<br>\n",
        "<br>\n",
        "Example:\n",
        "$$d = \\sqrt{ (x_2-x_1)^2 + (y_2-y_1)^2}$$\n",
        "<br>\n",
        "$$d = \\textrm{distance}$$\n",
        "$$(x_2-x_1) = \\textrm{coordinates of the first point}$$\n",
        "$$(y_2-y_1) = \\textrm{coordinates of the second point}$$"
      ],
      "metadata": {
        "id": "uyEEOejNqZ5E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQQzEhFiqZWS"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manhattan Distance\n",
        "\n",
        "The Manhattan distance, often called Taxicab distance or City Block distance, calculates the distance between real-valued vectors. Imagine vectors that describe objects on a uniform grid such as a chessboard. Manhattan distance then refers to the distance between two vectors if they could only move right angles. There is no diagonal movement involved in calculating the distance.\n",
        "\n",
        "Disadvantages\n",
        "Although Manhattan distance seems to work okay for high-dimensional data, it is a measure that is somewhat less intuitive than euclidean distance, especially when using in high-dimensional data.\n",
        "\n",
        "Moreover, it is more likely to give a higher distance value than euclidean distance since it does not the shortest path possible. This does not necessarily give issues but is something you should take into account.\n",
        "\n",
        "Use Cases\n",
        "When your dataset has discrete and/or binary attributes, Manhattan seems to work quite well since it takes into account the paths that realistically could be taken within values of those attributes. Take Euclidean distance, for example, would create a straight line between two vectors when in reality this might not actually be possible.\n",
        "\n",
        "$$D(x,y) = \\sqrt{ \\sum_{i=1}^{k}|x_i-y_i|^2}$$\n",
        "\n",
        "Example:\n",
        "$$d = {|x_2-x_1| + |y_2-y_1|}$$\n",
        "$$d = \\textrm{distance}$$\n",
        "$$|x_2-x_1| = \\textrm{coordinates of the first point}$$\n",
        "$$|y_2-y_1| = \\textrm{coordinates of the second point}$$"
      ],
      "metadata": {
        "id": "mGpnlF-8sFC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WckoZl2isAjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Minkowski Distance\n",
        "Minkowski distance is a bit more intricate measure than most. It is a metric used in Normed vector space (n-dimensional real space), which means that it can be used in a space where distances can be represented as a vector that has a length.\n",
        "\n",
        "This measure has three requirements:\n",
        "\n",
        "Zero Vector — The zero vector has a length of zero whereas every other vector has a positive length. For example, if we travel from one place to another, then that distance is always positive. However, if we travel from one place to itself, then that distance is zero.\n",
        "Scalar Factor — When you multiple the vector with a positive number its length is changed whilst keeping its direction. For example, if we go a certain distance in one direction and add the same distance, the direction does not change.\n",
        "Triangle Inequality — The shortest distance between two points is a straight line.\n",
        "\n",
        "The formula for the Minkowski distance is shown below:\n",
        "\n",
        "$$D(x,y) = \\left({\\sum_{i=1}^{k}|x_i-y_i|^{p}}\\right)^{\\dfrac{1}{p}}$$\n",
        "\n",
        "Most interestingly about this distance measure is the use of parameter $p$. We can use this parameter to manipulate the distance metrics to closely resemble others.\n",
        "\n",
        "Common values of $p$ are:\n",
        "\n",
        "$p$=1 — Manhattan distance <br>\n",
        "$p$=2 — Euclidean distance <br>\n",
        "$p$=$∞$ — Chebyshev distance <br>\n",
        "**Disadvantages**<br>\n",
        "Minkowski has the same disadvantages as the distance measures they represent, so a good understanding of metrics like Manhattan, Euclidean, and Chebyshev distance is extremely important.\n",
        "\n",
        "Moreover, the parameter p can actually be troublesome to work with as finding the right value can be quite computationally inefficient depending on your use-case.\n",
        "\n",
        "**Use Cases**<br>\n",
        "The upside to p is the possibility to iterate over it and find the distance measure that works best for your use case. It allows you a huge amount of flexibility over your distance metric, which can be a huge benefit if you are closely familiar with p and many distance measures."
      ],
      "metadata": {
        "id": "6GpwevrbwmlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9_fTd8QwwnHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cosine Similarity\n",
        "Cosine similarity has often been used as a way to counteract Euclidean distance’s problem with high dimensionality. The cosine similarity is simply the cosine of the angle between two vectors. It also has the same inner product of the vectors if they were normalized to both have length one.\n",
        "\n",
        "Two vectors with exactly the same orientation have a cosine similarity of 1, whereas two vectors diametrically opposed to each other have a similarity of -1. Note that their magnitude is not of importance as this is a measure of orientation.\n",
        "\n",
        "$$D(x,y) = \\cos(\\theta) = \\dfrac{x * y}{\\|x\\| \\|y\\|}$$\n",
        "\n",
        "**Disadvantages**\n",
        "\n",
        "One main disadvantage of cosine similarity is that the magnitude of vectors is not taken into account, merely their direction. In practice, this means that the differences in values are not fully taken into account. If you take a recommender system, for example, then the cosine similarity does not take into account the difference in rating scale between different users.\n",
        "\n",
        "**Use Cases**\n",
        "\n",
        "We use cosine similarity often when we have high-dimensional data and when the magnitude of the vectors is not of importance. For text analyses, this measure is quite frequently used when the data is represented by word counts. For example, when a word occurs more frequently in one document over another this does not necessarily mean that one document is more related to that word. It could be the case that documents have uneven lengths and the magnitude of the count is of less importance. Then, we can best be using cosine similarity which disregards magnitude."
      ],
      "metadata": {
        "id": "9hNvw8K8uPK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DyQj6gObvOsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Dh4trAolvPgG"
      }
    }
  ]
}